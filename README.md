# ğŸŠ SapBERT-KO-EN

- í•œêµ­ì–´ ëª¨ë¸ì„ ì´ìš©í•œ **SapBERT**(Self-alignment pretraining for BERT)ì…ë‹ˆë‹¤.
- í•œÂ·ì˜ ì˜ë£Œ ìš©ì–´ ì‚¬ì „ì¸ KOSTOMì„ ì‚¬ìš©í•´ í•œêµ­ì–´ ìš©ì–´ì™€ ì˜ì–´ ìš©ì–´ ì •ë ¬í•©ë‹ˆë‹¤.
- ì°¸ê³ : [SapBERT](https://aclanthology.org/2021.naacl-main.334.pdf), [Original Code](https://github.com/cambridgeltl/sapbert) 

&nbsp;

## 1. SapBERT-KO-EN

- SapBERTëŠ” ìˆ˜ë§ì€ **ì˜ë£Œ ë™ì˜ì–´**ë¥¼ ë™ì¼í•œ ì˜ë¯¸ë¡œ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ì‚¬ì „ í•™ìŠµ ë°©ë²•ë¡ ì…ë‹ˆë‹¤.
- Multi-Similarity Lossë¥¼ ì´ìš©í•´ **ë™ì¼í•œ ì˜ë£Œ ì½”ë“œ**ë¥¼ ì§€ë‹Œ ìš©ì–´ ê°„ì˜ ìœ ì‚¬ë„ë¥¼ í‚¤ìš°ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.

<p align="center">
<img src="sapbert_ko_en.PNG" alt="example image" width="500" height="250"/>
</p>

- í•œêµ­ ì˜ë£Œ ê¸°ë¡ì€ **í•œÂ·ì˜ í˜¼ìš©ì²´**ë¡œ ì´ë£¨ì–´ì ¸ ìˆì–´ í•œÂ·ì˜ ìš©ì–´ ê°„ì˜ ë™ì˜ì–´ê¹Œì§€ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.  
- **SapBERT-KO-EN**ëŠ” ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ í•œêµ­ì–´ ìš©ì–´ì™€ ì˜ì–´ ìš©ì–´ë¥¼ ëª¨ë‘ ì •ë ¬í•œ ëª¨ë¸ì…ë‹ˆë‹¤.  

&nbsp;&nbsp;&nbsp;&nbsp;(â€»ìœ„ ê·¸ë¦¼ì€ )

## 2. Model Structure

- ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ Bi-Encoder êµ¬ì¡°ë¥¼ **Single-Encoder êµ¬ì¡°**ë¡œ ë³€ê²½í–ˆìŠµë‹ˆë‹¤. [\[code\]]()
- Pytorch Metric Learning íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  Multi Simliarity Lossë¥¼ ì§ì ‘ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤. [\[code\]]()
  

## 3. Training Data
- ì˜ë£Œ ìš©ì–´ ì‚¬ì „ìœ¼ë¡œ, ì˜ì–´ ì¤‘ì‹¬ì˜ UMLS ëŒ€ì‹  í•œêµ­ì–´ ì¤‘ì‹¬ì˜ **KOSTOM**ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.   
- KOSTOMì€ ëª¨ë“  í•œêµ­ì–´ ìš©ì–´ì— ëŒ€ì‘í•˜ëŠ” ì˜ì–´ ìš©ì–´ ë° ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ ì˜ë£Œ ì½”ë“œë¥¼ í•¨ê»˜ ì œì‹œí•©ë‹ˆë‹¤.
- Pre-processingì„ í†µí•´ ë™ì¼í•œ ì½”ë“œë¥¼ ì§€ë‹Œ ìš©ì–´ë“¤ì„ 'ìŒ(pair)'ìœ¼ë¡œ êµ¬ì„±í•´ í•™ìŠµ ë°ì´í„°ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.
```
sent0, sent1, label
ê°„ê²½í™”, Liver Cirrhosis, C0023890
ê°„ê²½í™”, Hepatic Cirrhosis, C0023890
Liver Cirrhosis, Hepatic Cirrhosis, C0023890
...
```

## 4. Implementation

**(1) Pre-processing**
- '' íŒŒì¼ì„ ì´ìš©í•´ KOSTOM ì„ í•™ìŠµì„ ìœ„í•œ ë°ì´í„° ì…‹ìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. [\[code\]]()

**(2) Training**
- train ë””ë ‰í† ë¦¬ì˜ ì‰˜ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì´ìš©í•´ ëª¨ë¸ì„ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ì‰˜ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ë² ì´ìŠ¤ ëª¨ë¸ ë° í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ ì§ì ‘ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
```
cd train
sh run_train.sh
```
## 5. Training Example
- ëª¨ë¸ í•™ìŠµì— í™œìš©í•œ ë² ì´ìŠ¤ ëª¨ë¸ ë° í•˜ì´í¼ íŒŒë¼ë¯¸í„°ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
  - Model : klue/bert-base
  - Epochs : 1
  - Batch Size : 64
  - Max Length : 64
  - Dropout : 0.1
  - Pooler : 'cls'
  - Eval Step : 100
  - Threshold : 0.8
  - Scale Positive Sample : 1
  - Scale Negative Sample : 60

- ì–´íœ˜ ì‚¬ì „ì— ì˜ì–´ í† í°ì´ í¬í•¨ë˜ì—ˆë‹¤ë©´ **í•œêµ­ì–´ ëª¨ë¸**ë„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  - í•™ìŠµ ì „ í•œÂ·ì˜ ì˜ë£Œ ìš©ì–´ë¥¼ ì–´íœ˜ ì‚¬ì „ì— ì¶”ê°€í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥í•©ë‹ˆë‹¤.
  - ë‹¤êµ­ì–´ ëª¨ë¸ì¸ XLM-RoBERTa ëª¨ë¸ë„ ì½”ë“œ ìˆ˜ì • ì—†ì´ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤. 

- í•™ìŠµí•œ ëª¨ë¸ì€ ë‹¤ìŒê³¼ ê°™ì´ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
```python
import numpy as np
from transformers import AutoModel, AutoTokenizer

model_path = 'snumin44/simcse-ko-bert-supervised'
model = AutoModel.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path)

query = 'ë‚´ì¼ ì•„ì¹¨ì— ë¹„ê°€ ì˜¬ê¹Œìš”?'

targets = [
    'ë‚´ì¼ ì•„ì¹¨ì— ìš°ì‚°ì„ ì±™ê²¨ì•¼ í•©ë‹ˆë‹¤.',
    'ì–´ì œ ì €ë…ì—ëŠ” ë¹„ê°€ ë§ì´ ë‚´ë ¸ìŠµë‹ˆë‹¤.',
    'ì²­ê³„ì²œì€ ëŒ€í•œë¯¼êµ­ ì„œìš¸ì— ìˆìŠµë‹ˆë‹¤.',
    'ì´ë²ˆ ì£¼ë§ì— ì¶•êµ¬ ëŒ€í‘œíŒ€ ê²½ê¸°ê°€ ìˆìŠµë‹ˆë‹¤.',
    'ì €ëŠ” ë§¤ì¼ ì•„ì¹¨ ì¼ì° ì¼ì–´ë‚˜ ì±…ì„ ì½ìŠµë‹ˆë‹¤.'
]

query_feature = tokenizer(query, return_tensors='pt')
query_outputs = model(**query_feature, return_dict=True)
query_embeddings = query_outputs.pooler_output.detach().numpy().squeeze()

def cos_sim(A, B):
    return np.dot(A, B) / (np.linalg.norm(A) * np.linalg.norm(B))

for idx, target in enumerate(targets):
    target_feature = tokenizer(target, return_tensors='pt')
    target_outputs = model(**target_feature, return_dict=True)
    target_embeddings = target_outputs.pooler_output.detach().numpy().squeeze()
    similarity = cos_sim(query_embeddings, target_embeddings)
    print(f"Similarity between query and target {idx}: {similarity:.4f}")
```
```
Similarity between query and target 0: 0.7864
Similarity between query and target 1: 0.5695
Similarity between query and target 2: 0.2646
Similarity between query and target 3: 0.3055
Similarity between query and target 4: 0.3738
```

## 6. Fine-tuning Example


