# ğŸŠ SapBERT-KO-EN

- í•œêµ­ì–´ ëª¨ë¸ì„ ì´ìš©í•œ **SapBERT**(Self-alignment pretraining for BERT)ì…ë‹ˆë‹¤.
- í•œÂ·ì˜ ì˜ë£Œ ìš©ì–´ ì‚¬ì „ì¸ KOSTOMì„ ì‚¬ìš©í•´ í•œêµ­ì–´ ìš©ì–´ì™€ ì˜ì–´ ìš©ì–´ ì •ë ¬í•©ë‹ˆë‹¤.
- ì°¸ê³ : [SapBERT](https://aclanthology.org/2021.naacl-main.334.pdf), [Original Code](https://github.com/cambridgeltl/sapbert) 

&nbsp;

## 1. SapBERT-KO-EN

- SapBERTëŠ” ìˆ˜ë§ì€ **ì˜ë£Œ ë™ì˜ì–´**ë¥¼ ë™ì¼í•œ ì˜ë¯¸ë¡œ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ì‚¬ì „ í•™ìŠµ ë°©ë²•ë¡ ì…ë‹ˆë‹¤.
- Multi-Similarity Lossë¥¼ ì´ìš©í•´ **ë™ì¼í•œ ì˜ë£Œ ì½”ë“œ**ë¥¼ ì§€ë‹Œ ìš©ì–´ ê°„ì˜ ìœ ì‚¬ë„ë¥¼ í‚¤ìš°ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.

<p align="center">
<img src="sapbert_ko_en.PNG" alt="example image" width="500" height="250"/>
</p>

- í•œêµ­ ì˜ë£Œ ê¸°ë¡ì€ **í•œÂ·ì˜ í˜¼ìš©ì²´**ë¡œ ì´ë£¨ì–´ì ¸ ìˆì–´ í•œÂ·ì˜ ìš©ì–´ ê°„ì˜ ë™ì˜ì–´ê¹Œì§€ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.  
- **SapBERT-KO-EN**ëŠ” ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ í•œêµ­ì–´ ìš©ì–´ì™€ ì˜ì–´ ìš©ì–´ë¥¼ ëª¨ë‘ ì •ë ¬í•œ ëª¨ë¸ì…ë‹ˆë‹¤.  

&nbsp;&nbsp;&nbsp;&nbsp;(â€»ìœ„ ê·¸ë¦¼ì€ )

## 2. Model Structure

- ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ Bi-Encoder êµ¬ì¡°ë¥¼ **Single-Encoder êµ¬ì¡°**ë¡œ ë³€ê²½í–ˆìŠµë‹ˆë‹¤. [\[code\]](https://github.com/snumin44/SapBERT-KO-EN/blob/main/src/model.py)
- Pytorch Metric Learning íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  Multi Simliarity Lossë¥¼ ì§ì ‘ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤. [\[code\]](https://github.com/snumin44/SapBERT-KO-EN/blob/main/src/loss.py)
  

## 3. Training Data
- ì˜ë£Œ ìš©ì–´ ì‚¬ì „ìœ¼ë¡œ, ì˜ì–´ ì¤‘ì‹¬ì˜ UMLS ëŒ€ì‹  í•œêµ­ì–´ ì¤‘ì‹¬ì˜ **KOSTOM**ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.   
- KOSTOMì€ ëª¨ë“  í•œêµ­ì–´ ìš©ì–´ì— ëŒ€ì‘í•˜ëŠ” ì˜ì–´ ìš©ì–´ ë° ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ ì˜ë£Œ ì½”ë“œë¥¼ í•¨ê»˜ ì œì‹œí•©ë‹ˆë‹¤.
- Pre-processingì„ í†µí•´ ë™ì¼í•œ ì½”ë“œë¥¼ ì§€ë‹Œ ìš©ì–´ë“¤ì„ 'ìŒ(pair)'ìœ¼ë¡œ êµ¬ì„±í•´ í•™ìŠµ ë°ì´í„°ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.
```
sent0, sent1, label
ê°„ê²½í™”, Liver Cirrhosis, C0023890
ê°„ê²½í™”, Hepatic Cirrhosis, C0023890
Liver Cirrhosis, Hepatic Cirrhosis, C0023890
...
```

## 4. Implementation

**(1) Pre-processing**
- **'kostom_preprocessing.ipynb'** ë¥¼ ì´ìš©í•´ KOSTOMì„ í•™ìŠµì„ ìœ„í•œ ë°ì´í„° ì…‹ìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. [\[code\]](https://github.com/snumin44/SapBERT-KO-EN/tree/main/data)

**(2) Training**
- train ë””ë ‰í† ë¦¬ì˜ ì‰˜ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì´ìš©í•´ ëª¨ë¸ì„ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. [\[code\]](https://github.com/snumin44/SapBERT-KO-EN/tree/main/train)
- ì‰˜ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ë² ì´ìŠ¤ ëª¨ë¸ ë° í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ ì§ì ‘ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
```
cd train
sh run_train.sh
```
## 5. Training Example
- ëª¨ë¸ í•™ìŠµì— í™œìš©í•œ ë² ì´ìŠ¤ ëª¨ë¸ ë° í•˜ì´í¼ íŒŒë¼ë¯¸í„°ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
  - Model : klue/bert-base
  - Epochs : 1
  - Batch Size : 64
  - Max Length : 64
  - Dropout : 0.1
  - Pooler : 'cls'
  - Eval Step : 100
  - Threshold : 0.8
  - Scale Positive Sample : 1
  - Scale Negative Sample : 60

- ì–´íœ˜ ì‚¬ì „ì— ì˜ì–´ í† í°ì´ í¬í•¨ë˜ì—ˆë‹¤ë©´ **í•œêµ­ì–´ ëª¨ë¸**ë„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  - í•™ìŠµ ì „ í•œÂ·ì˜ ì˜ë£Œ ìš©ì–´ë¥¼ ì–´íœ˜ ì‚¬ì „ì— ì¶”ê°€í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥í•©ë‹ˆë‹¤.
  - ë‹¤êµ­ì–´ ëª¨ë¸ì¸ XLM-RoBERTa ëª¨ë¸ë„ ì½”ë“œ ìˆ˜ì • ì—†ì´ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.
    
- í•™ìŠµ ëª¨ë¸ì˜ ì²´í¬ í¬ì¸íŠ¸ëŠ” [HuggingFace ë ˆí¬](https://huggingface.co/snumin44/sap-bert-ko-en)ì—ì„œ ë‹¤ìš´ë¡œë“œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

```python
import numpy as np
from transformers import AutoModel, AutoTokenizer

model_path = 'snumin44/sap-bert-ko-en'
model = AutoModel.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path)

query = 'ê°„ê²½í™”'

targets = [
    'liver cirrhosis',
    'ê°„ê²½ë³€',
    'liver cancer',
    'ê°„ì•”',
    'brain tumor',
    'ë‡Œì¢…ì–‘'
]

query_feature = tokenizer(query, return_tensors='pt')
query_outputs = model(**query_feature, return_dict=True)
query_embeddings = query_outputs.pooler_output.detach().numpy().squeeze()

def cos_sim(A, B):
    return np.dot(A, B) / (np.linalg.norm(A) * np.linalg.norm(B))

for idx, target in enumerate(targets):
    target_feature = tokenizer(target, return_tensors='pt')
    target_outputs = model(**target_feature, return_dict=True)
    target_embeddings = target_outputs.pooler_output.detach().numpy().squeeze()
    similarity = cos_sim(query_embeddings, target_embeddings)
    print(f"Similarity between query and target {idx}: {similarity:.4f}")
```
```
Similarity between query and target 0: 0.7145
Similarity between query and target 1: 0.7186
Similarity between query and target 2: 0.6183
Similarity between query and target 3: 0.6972
Similarity between query and target 4: 0.3929
Similarity between query and target 5: 0.4260
```

## 6. Fine-tuning Example

- ë…¼ë¬¸ì—ì„œëŠ” **Medical Entity Linking** í…ŒìŠ¤í¬ì— ëŒ€í•´ Fine-tuning ì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.
- ë‹¤ìŒê³¼ ê°™ì´ **Medical QA** ë°ì´í„° ì…‹ì„ ì´ìš©í•´ ê²€ìƒ‰ ëª¨ë¸ë¡œ Fine-tuning í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥í•©ë‹ˆë‹¤.
  - Medical QA ë°ì´í„° ì…‹ìœ¼ë¡œ AI Hubì˜ **'ì´ˆê±°ëŒ€ AI í—¬ìŠ¤ì¼€ì–´ ì§ˆì˜ì‘ë‹µ ë°ì´í„°'** ë°ì´í„° ì…‹ì„ ì´ìš©í–ˆìŠµë‹ˆë‹¤.
  - ë² ì´ìŠ¤ ëª¨ë¸ì€ 'snumin44/sap-bert-ko-en'ì„ ì‚¬ìš©í–ˆê³ , [DPR-KO ì½”ë“œ](https://github.com/snumin44/DPR-KO)ë¡œ Fine-tuningì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.   

<p align="center">
<img src="medical_search.gif" width="480" height="280" alt="Medical Search Engine (Demo)">
</p>
